"""
6ë‹¨ê³„: ë‹µë³€ ìƒì„± ë¡œì§
"""

import logging
from typing import Dict, List
from langchain_core.prompts import ChatPromptTemplate
from models.state import MeetingQAState
from utils.content_filter import detect_content_filter, create_safe_response

logger = logging.getLogger(__name__)

class AnswerGenerator:
    """ë‹µë³€ ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def _deduplicate_sources(self, sources: List[Dict]) -> List[Dict]:
        """script_id ê¸°ì¤€ ìµœê³  ê´€ë ¨ë„ë§Œ ìœ ì§€"""
        seen = {}
        for source in sources:
            script_id = source["script_id"]
            if script_id not in seen or source["relevance_score"] > seen[script_id]["relevance_score"]:
                seen[script_id] = source
        return list(seen.values())
    
    def generate_final_answer(self, state: MeetingQAState) -> MeetingQAState:
        """7ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„±"""
        try:
            user_question = state.get("user_question", "")
            relevant_summaries = state.get("relevant_summaries", [])
            relevant_chunks = state.get("relevant_chunks", [])
            conversation_memory = state.get("conversation_memory", "")
            
            if not user_question:
                raise ValueError("ì‚¬ìš©ì ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
            context_parts = []
            
            # ìš”ì•½ë³¸ ì¶”ê°€ (í•­ìƒ í¬í•¨)
            for summary in relevant_summaries[:3]:
                summary_text = summary.get('summary_text', '')
                if summary_text:
                    context_parts.append(f"[ìš”ì•½ë³¸] {summary_text}")
            
            # ê´€ë ¨ ì²­í¬ ì¶”ê°€ (ìˆëŠ” ê²½ìš°ì—ë§Œ)
            for chunk in relevant_chunks[:5]:
                chunk_text = chunk.get('chunk_text', '')
                if chunk_text:
                    context_parts.append(f"[ì›ë³¸] {chunk_text}")
            
            # ì›ë³¸ ì²­í¬ê°€ ì—†ìœ¼ë©´ ìš”ì•½ë³¸ë§Œìœ¼ë¡œ ë‹µë³€ ìƒì„±
            if not relevant_chunks and relevant_summaries:
                logger.info("ì›ë³¸ ì²­í¬ê°€ ì—†ì–´ì„œ ìš”ì•½ë³¸ë§Œìœ¼ë¡œ ë‹µë³€ ìƒì„±")
            
            context = "\n\n".join(context_parts)
            
            # ëŒ€í™” ë©”ëª¨ë¦¬ í¬í•¨í•œ ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
            memory_context = f"\n\nì´ì „ ëŒ€í™” ë§¥ë½: {conversation_memory}" if conversation_memory else ""
            
            answer_prompt = ChatPromptTemplate.from_template(
                '''ë‹¹ì‹ ì€ íšŒì˜ë¡ ê¸°ë°˜ â€œì¶”ì¶œí˜•â€ QA ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
                **í˜•ì‹ ê·œì¹™:**
                - ë‹µë³€ì€ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ë˜, ê·¼ê±°ê°€ ë˜ëŠ” ì¸ìš©ë¬¸ì€ ë°˜ë“œì‹œ ë³„ë„ ì¤„ì— ì‘ì„±í•˜ì„¸ìš”.
                - ì¸ìš©ë¬¸ì€ ("ì¸ìš©ë‚´ìš©", í™”ìëª…) í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ê³ , ë³¸ë¬¸ê³¼ í•œ ì¤„ ë„ì›Œì„œ êµ¬ë¶„í•˜ì„¸ìš”.
                - ê° ì£¼ìš” í¬ì¸íŠ¸ë§ˆë‹¤ ê´€ë ¨ ì¸ìš©ë¬¸ì„ ë°”ë¡œ ì•„ë˜ì— ë°°ì¹˜í•˜ì„¸ìš”.
                - ìµœì¢… ë‹µë³€ì€ 5ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ êµ¬ì„±í•˜ì„¸ìš”.
                - ë‹µë³€ì€ "~ì…ë‹ˆë‹¤.", "~ìŠµë‹ˆë‹¤."ì™€ ê°™ì€ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.

                **ì˜ˆì‹œ í˜•ì‹:**
                ëª…í•¨ êµí™˜ ì‹œì—ëŠ” ì„œì„œ ê°€ë²¼ìš´ ì¸ì‚¬ì™€ í•¨ê»˜ ê¸€ììª½ì„ ìƒëŒ€ë°©ì—ê²Œ í–¥í•˜ê²Œ í•˜ì—¬ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.

                ("ëª…í•¨ì€ ë‚´ ì–¼êµ´ì´ë¼ê³  ìƒê°í•´ ì£¼ì‹œë©´ ë¼ìš”â€¦ê¸€ììª½ì„ ìƒëŒ€ë°©ì„ í–¥í•˜ê²Œ ì†ê°€ë½ìœ¼ë¡œ ê°€ë¦¬ì§€ ì•Šê²Œ ì˜†ë¶€ë¶„ì„ ì¡ê³ ", í™”ì01)

                êµí™˜ ìˆœì„œëŠ” ì•„ë«ì‚¬ëŒì´ ìœ—ì‚¬ëŒì—ê²Œ ë¨¼ì € ì£¼ëŠ” ê²ƒì´ ì˜ˆì˜ì…ë‹ˆë‹¤.

                ("ê±´ë„¤ëŠ” ìˆœì„œëŠ” ì´ì œ ì•„ë«ì‚¬ëŒì´ ìœ—ì‚¬ëŒí•œí…Œ ë¨¼ì € í•˜ê³ â€¦ì˜¤ì‹œëŠ” ë¶„ë“¤ì´ ë¨¼ì € ì´ë ‡ê²Œ ì£¼ì„¸ìš”", í™”ì01)
                '''
            )
            
            formatted_prompt = answer_prompt.format(
                question=user_question,
                memory_context=memory_context,
                context=context
            )
            
            response = self.llm.invoke(formatted_prompt)
            final_answer = response.content.strip()
            
            # fallback ë©”ì‹œì§€ ì²˜ë¦¬ ì œê±° (ë‹¨ìˆœí•œ ì—ëŸ¬ ì²˜ë¦¬ë¡œ ë³€ê²½)
            
            # ì¶œì²˜ ì •ë³´ ìƒì„± (ì²­í‚¹ ê´€ë ¨ ì •ë³´ë§Œ)
            sources = []
            for chunk in relevant_chunks[:10]:  # ë” ë§ì€ ì²­í¬ì—ì„œ ì„ ë³„
                source = {
                    "script_id": chunk["script_id"],
                    "chunk_index": chunk["chunk_index"],
                    "relevance_score": chunk["relevance_score"]
                }
                sources.append(source)
            
            # script_id ê¸°ì¤€ ì¤‘ë³µ ì œê±° (ìµœê³  ê´€ë ¨ë„ë§Œ ìœ ì§€)
            sources = self._deduplicate_sources(sources)
            
            # ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 5ê°œë§Œ ìœ ì§€
            sources.sort(key=lambda x: x["relevance_score"], reverse=True)
            sources = sources[:5]

            # ì‹¤ì œ ì‚¬ìš©ëœ ë¬¸ì„œ ID ê³„ì‚°
            used_script_ids = sorted({s["script_id"] for s in sources})
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
            confidence_score = min(0.9, max(0.1, 
                sum(chunk["relevance_score"] for chunk in relevant_chunks[:3]) / 3
            )) if relevant_chunks else 0.1
            
            logger.info(f"ë‹µë³€ ìƒì„± ì™„ë£Œ (ì¶œì²˜ ì¤‘ë³µ ì œê±° ì ìš©): ì‹ ë¢°ë„ {confidence_score:.2f}, ì¶œì²˜ {len(sources)}ê°œ")
            
            # ì œëª© ì •ë³´ëŠ” script_metadataë¡œ ë³„ë„ ì œê³µ
            script_metadata = state.get("script_metadata", {})
            
            # ìµœì¢… ì‘ë‹µ state êµ¬ì„±
            final_state = {
                **state,
                "context_chunks": context_parts,
                "final_answer": final_answer,
                "sources": sources,  # ì²­í‚¹ ê´€ë ¨ ì •ë³´ë§Œ
                "script_metadata": script_metadata,  # ì œëª© ì •ë³´ëŠ” ë³„ë„
                "used_script_ids": used_script_ids,
                "confidence_score": confidence_score,
                "current_step": "completed"
            }
            
            # ğŸ¯ í”„ë¡ íŠ¸ì—”ë“œìš© ìµœì¢… State êµ¬ì¡° ë¡œê·¸ ì¶œë ¥
            import json
            logger.info("=" * 80)
            logger.info("ğŸ¯ ìµœì¢… ë‹µë³€ STATE êµ¬ì¡° (í”„ë¡ íŠ¸ì—”ë“œìš©)")
            logger.info("=" * 80)
            
            # í•µì‹¬ ì‘ë‹µ ë°ì´í„°ë§Œ ì¶”ì¶œ
            response_structure = {
                "user_question": final_state.get("user_question", ""),
                "final_answer": final_state.get("final_answer", ""),
                "confidence_score": final_state.get("confidence_score", 0.0),
                "sources": final_state.get("sources", []),
                "script_metadata": final_state.get("script_metadata", {}),
                "used_script_ids": final_state.get("used_script_ids", []),
                "current_step": final_state.get("current_step", ""),
                "context_chunks": final_state.get("context_chunks", []),
                # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                "total_chunks_analyzed": len(final_state.get("chunked_scripts", [])),
                "selected_chunks_count": len(final_state.get("relevant_chunks", [])),
                "memory_context": final_state.get("conversation_memory", "")
            }
            
            logger.info("ğŸ“‹ ì‘ë‹µ êµ¬ì¡°:")
            logger.info(json.dumps(response_structure, ensure_ascii=False, indent=2))
            
            # Sourcesì™€ script_metadata êµ¬ì¡° ì„¤ëª…
            if sources:
                logger.info("ğŸ“„ Sources êµ¬ì¡° (ì²­í‚¹ ê´€ë ¨ ì •ë³´ë§Œ):")
                for i, source in enumerate(sources[:2]):  # ì²˜ìŒ 2ê°œë§Œ ì˜ˆì‹œë¡œ
                    logger.info(f"  Source {i+1}:")
                    logger.info(f"    - script_id: {source.get('script_id', '')}")
                    logger.info(f"    - chunk_index: {source.get('chunk_index', 0)}")
                    logger.info(f"    - relevance_score: {source.get('relevance_score', 0.0):.3f}")
            
            if script_metadata:
                logger.info("ğŸ“‹ Script Metadata êµ¬ì¡° (ì œëª© ì •ë³´):")
                first_key = list(script_metadata.keys())[0]
                logger.info(f"  '{first_key}': {script_metadata[first_key]}")
            
            logger.info("=" * 80)
            
            return final_state
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            
            # Azure ì½˜í…ì¸  í•„í„° ê°ì§€ (ì˜¤ë¥˜ ì½”ë“œ ìš°ì„ )
            filter_info = detect_content_filter(e)
            if filter_info['is_filtered']:
                logger.warning(f"ë‹µë³€ ìƒì„± ì¤‘ ì½˜í…ì¸  í•„í„° ê°ì§€: {filter_info}")
                return create_safe_response(state, 'generate_answer', filter_info)
            
            # ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ì²˜ë¦¬
            return {
                **state,
                "error_message": f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}",
                "current_step": "generate_answer_failed"
            }
    
    def improve_answer(self, state: MeetingQAState) -> MeetingQAState:
        """ë‹µë³€ ê°œì„ """
        try:
            question = state.get("processed_question", "")
            current_answer = state.get("final_answer", "")
            quality_score = state.get("answer_quality_score", 0)
            relevant_summaries = state.get("relevant_summaries", [])
            relevant_chunks = state.get("relevant_chunks", [])
            improvement_attempts = int(state.get("improvement_attempts") or 0) + 1
            
            # ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
            context_parts = []
            
            # ìš”ì•½ë³¸ ì¶”ê°€
            for summary in relevant_summaries[:3]:
                context_parts.append(f"[ìš”ì•½ë³¸] {summary.get('summary_text', '')}")
            
            # ê´€ë ¨ ì²­í¬ ì¶”ê°€
            for chunk in relevant_chunks[:5]:
                context_parts.append(f"[ì›ë³¸] {chunk.get('chunk_text', '')}")
            
            context = "\n\n".join(context_parts)
            
            # ê°œì„ ëœ ë‹µë³€ ìƒì„±
            improvement_prompt = f"""
            ì´ì „ ë‹µë³€ì˜ í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤ (ì ìˆ˜: {quality_score}/5).
            ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ë” ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ìœ¼ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”.

            **ì—„ê²©í•œ ê°œì„  ê·œì¹™:**
            1. **ì •í™•ì„±**: ì œê³µëœ 'ì°¸ê³  ìë£Œ'ì— ê¸°ë°˜í•˜ì—¬, ì´ì „ ë‹µë³€ì˜ ë¶€ì •í™•í•œ ë¶€ë¶„ì„ ëª¨ë‘ ìˆ˜ì •í•˜ì„¸ìš”.
            2. **ëª…í™•ì„±**: ë¶ˆë¶„ëª…í–ˆë˜ ë‚´ìš©ì´ë‚˜ ëª¨í˜¸í•œ í‘œí˜„ì„ ì œê±°í•˜ê³ , í•µì‹¬ ì •ë³´ë¥¼ ëª…í™•í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.
            3. **ê°„ê²°ì„±**: ìµœì¢… ë‹µë³€ì€ 5ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ë˜, í•„ìš”í•œ ì •ë³´ëŠ” ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”.
            4. **ì¶œì²˜ ëª…ì‹œ**: ë‹µë³€ ë‚´ì— ê·¼ê±°ê°€ ë˜ëŠ” 'ì°¸ê³  ìë£Œ'ì˜ í•µì‹¬ ë¬¸êµ¬ë¥¼ ì§ì ‘ ì¸ìš©í•˜ì—¬ ì‹ ë¢°ë„ë¥¼ ë†’ì´ì„¸ìš”.
            5. **ì¶”ì¸¡ ê¸ˆì§€**: 'ì°¸ê³  ìë£Œ'ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
            6. **ì¸ìš©ë¬¸ í˜•ì‹**: ë‹µë³€ì€ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ë˜, ê·¼ê±°ê°€ ë˜ëŠ” ì¸ìš©ë¬¸ì€ ë°˜ë“œì‹œ ë³„ë„ ì¤„ì— ì‘ì„±í•˜ì„¸ìš”.
                            - ì¸ìš©ë¬¸ì€ ("ì¸ìš©ë‚´ìš©", í™”ìëª…) í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ê³ , ë³¸ë¬¸ê³¼ í•œ ì¤„ ë„ì›Œì„œ êµ¬ë¶„í•˜ì„¸ìš”.
            7. **ê·œì¹™ ì–¸ê¸‰ ê¸ˆì§€**: ë‹µë³€ ë‚´ìš©ì— ì´ ê·œì¹™ë“¤ì„ ë‹¤ì‹œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.

            ì§ˆë¬¸: {question}
            ì´ì „ ë‹µë³€: {current_answer}
            ì°¸ê³  ìë£Œ:
            {context}
            """
            response = self.llm.invoke(improvement_prompt)
            improved_answer = response.content.strip()
            
            # fallback ë©”ì‹œì§€ ì²˜ë¦¬ ì œê±° (ë‹¨ìˆœí•œ ì—ëŸ¬ ì²˜ë¦¬ë¡œ ë³€ê²½)
            
            return {
                **state,
                "final_answer": improved_answer,
                "improvement_attempts": improvement_attempts,
                "current_step": "answer_improved"
            }
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ê°œì„  ì‹¤íŒ¨: {str(e)}")
            
            # Azure ì½˜í…ì¸  í•„í„° ê°ì§€ (ì˜¤ë¥˜ ì½”ë“œ ìš°ì„ )
            filter_info = detect_content_filter(e)
            if filter_info['is_filtered']:
                logger.warning(f"ë‹µë³€ ê°œì„  ì¤‘ ì½˜í…ì¸  í•„í„° ê°ì§€: {filter_info}")
                safe_response = create_safe_response(state, 'improve_answer', filter_info)
                # improvement_attempts ì¶”ê°€
                safe_response["improvement_attempts"] = int(state.get("improvement_attempts") or 0) + 1
                return safe_response
            
            # ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ì²˜ë¦¬
            return {
                **state,
                "improvement_attempts": int(state.get("improvement_attempts") or 0) + 1,
                "current_step": "improvement_failed"
            }
