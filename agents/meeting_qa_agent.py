from langchain_openai import AzureChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.output_parsers import CommaSeparatedListOutputParser
from langgraph.graph import StateGraph, END
from typing import Literal

from models.state import MeetingQAState
from services.rag_client import RAGClient
from utils.text_processing import chunk_text, clean_text
from utils.embeddings import EmbeddingManager, find_most_relevant_chunks
from config.settings import AZURE_OPENAI_CONFIG, DEFAULT_CHUNK_SIZE, DEFAULT_CHUNK_OVERLAP, RAG_SERVICE_URL, MEETING_API_URL
import logging
import httpx

logger = logging.getLogger(__name__)

class MeetingQAAgent:
    """íšŒì˜ë¡ QA Agent"""
    
    def __init__(self):
        self.llm = AzureChatOpenAI(
            deployment_name=AZURE_OPENAI_CONFIG["deployment_name"],
            api_version=AZURE_OPENAI_CONFIG["api_version"],
            azure_endpoint=AZURE_OPENAI_CONFIG["endpoint"],
            api_key=AZURE_OPENAI_CONFIG["api_key"]
        )
        self.embedding_manager = EmbeddingManager()
        
        # Agent ê·¸ë˜í”„ êµ¬ì„±
        self.graph = self._build_graph()

    def should_improve_answer(self, state: MeetingQAState) -> str:
        """ë‹µë³€ ê°œì„  ì—¬ë¶€ ê²°ì •"""
        quality_score = state.get("answer_quality_score", 5)
        attempt_count = state.get("improvement_attempts", 0)
        
        print(f"ğŸ” ê°œì„  ì—¬ë¶€ ê²°ì •: ì ìˆ˜={quality_score}, ì‹œë„={attempt_count}")
        
        # ì´ë¯¸ ê°œì„ ì„ ì‹œë„í–ˆë‹¤ë©´ ë” ì´ìƒ ê°œì„ í•˜ì§€ ì•ŠìŒ
        if attempt_count >= 1:
            print("   â†’ ì´ë¯¸ 1íšŒ ê°œì„  ì‹œë„ ì™„ë£Œ, ì¢…ë£Œ")
            return "finish"
        
        if quality_score <= 3:
            print("   â†’ í’ˆì§ˆ ë‚®ìŒ, ê°œì„  ì§„í–‰")
            return "improve"
        else:
            print("   â†’ í’ˆì§ˆ ì–‘í˜¸, ì¢…ë£Œ")
            return "finish"
    
    def _build_graph(self) -> StateGraph:
        """Agent ê·¸ë˜í”„ êµ¬ì„±"""
        builder = StateGraph(MeetingQAState)
        
        # ë…¸ë“œ ì¶”ê°€
        builder.add_node("process_question", self.process_question)
        builder.add_node("search_rag", self.search_rag_summaries)
        builder.add_node("fetch_metadata", self.fetch_meeting_metadata)
        builder.add_node("fetch_scripts", self.fetch_original_scripts)
        builder.add_node("process_scripts", self.process_original_scripts)
        builder.add_node("select_chunks", self.select_relevant_chunks)
        builder.add_node("generate_answer", self.generate_final_answer)
        builder.add_node("evaluate_answer", self.evaluate_answer_quality)
        builder.add_node("improve_answer", self.improve_answer)
        
        # ì—£ì§€ ì—°ê²°
        builder.set_entry_point("process_question")
        builder.add_edge("process_question", "search_rag")
        builder.add_edge("search_rag", "fetch_metadata")
        builder.add_edge("fetch_metadata", "fetch_scripts")
        builder.add_edge("fetch_scripts", "process_scripts")
        builder.add_edge("process_scripts", "select_chunks")
        builder.add_edge("select_chunks", "generate_answer")
        builder.add_edge("generate_answer", "evaluate_answer")
        
        # ì¡°ê±´ë¶€ ì—£ì§€
        builder.add_conditional_edges(
            "evaluate_answer",
            self.should_improve_answer,
            {
                "improve": "improve_answer",
                "finish": END
            }
        )
        builder.add_edge("improve_answer", "evaluate_answer")
        
        return builder.compile()

    def summarize_conversation_history(self, state: MeetingQAState) -> MeetingQAState:
        """ì´ì „ ëŒ€í™” ìš”ì•½ ìƒì„±"""
        try:
            current_question = state.get("user_question", "")
            previous_memory = state.get("conversation_memory", "")
            conversation_count = state.get("conversation_count", 0)
            
            if conversation_count == 0:
                # ì²« ë²ˆì§¸ ëŒ€í™”
                return {
                    **state,
                    "conversation_memory": "",
                    "conversation_count": 1
                }
            
            # ì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ì§ˆë¬¸ì„ ìš”ì•½
            summary_prompt = f"""
            ì´ì „ ëŒ€í™” ìš”ì•½: {previous_memory}
            í˜„ì¬ ì§ˆë¬¸: {current_question}
            
            ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëŒ€í™”ì˜ ë§¥ë½ì„ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”.
            ì¤‘ìš”í•œ í‚¤ì›Œë“œì™€ ì£¼ì œë§Œ í¬í•¨í•˜ì—¬ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
            """
            
            response = self.llm.invoke(summary_prompt)
            new_memory = response.content.strip()
            
            return {
                **state,
                "conversation_memory": new_memory,
                "conversation_count": conversation_count + 1
            }
            
        except Exception as e:
            logger.error(f"ëŒ€í™” ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
            return {
                **state,
                "conversation_memory": state.get("conversation_memory", ""),
                "conversation_count": state.get("conversation_count", 0) + 1
                }

    def enhance_question_with_memory(self, state: MeetingQAState) -> MeetingQAState:
        """ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì§ˆë¬¸ ë³´ê°•"""
        try:
            original_question = state.get("user_question", "")
            memory = state.get("conversation_memory", "")
            
            if not memory:
                # ë©”ëª¨ë¦¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                return {
                    **state,
                    "processed_question": original_question
                }
            
            # ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì§ˆë¬¸ ë³´ê°•
            enhanced_prompt = f"""
            ì´ì „ ëŒ€í™” ë§¥ë½: {memory}
            í˜„ì¬ ì§ˆë¬¸: {original_question}
            
            ìœ„ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
            ì´ì „ ëŒ€í™”ì™€ì˜ ì—°ê´€ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì§ˆë¬¸ì„ ê°œì„ í•´ì£¼ì„¸ìš”.
            """
            
            response = self.llm.invoke(enhanced_prompt)
            enhanced_question = response.content.strip()
            
            return {
                **state,
                "processed_question": enhanced_question
            }
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ë³´ê°• ì‹¤íŒ¨: {str(e)}")
            return {
                **state,
                "processed_question": state.get("user_question", "")
            }
    
    async def run(self, initial_state: MeetingQAState) -> MeetingQAState:
        """Agent ì‹¤í–‰"""
        try:
            logger.info("Meeting QA Agent ì‹¤í–‰ ì‹œì‘")
            final_state = await self.graph.ainvoke(initial_state)
            logger.info("Meeting QA Agent ì‹¤í–‰ ì™„ë£Œ")
            return final_state
        except Exception as e:
            logger.error(f"Agent ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return {
                **initial_state,
                "error_message": f"Agent ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}",
                "current_step": "failed"
            }

    def evaluate_answer_quality(self, state: MeetingQAState) -> MeetingQAState:
        """ë‹µë³€ í’ˆì§ˆ í‰ê°€"""
        try:
            question = state.get("processed_question", "")
            answer = state.get("final_answer", "")
            context_chunks = state.get("context_chunks", [])
            
            if not answer:
                return {
                    **state,
                    "answer_quality_score": 1,
                    "current_step": "quality_evaluated"
                }
            
            # LLMì„ ì‚¬ìš©í•œ í’ˆì§ˆ í‰ê°€
            evaluation_prompt = f"""
            ë‹¤ìŒ ë‹µë³€ì˜ í’ˆì§ˆì„ 1-5ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
            
            ì§ˆë¬¸: {question}
            ë‹µë³€: {answer}
            
            í‰ê°€ ê¸°ì¤€:
            1ì : ì „í˜€ ê´€ë ¨ ì—†ëŠ” ë‹µë³€
            2ì : ê´€ë ¨ì€ ìˆì§€ë§Œ ë¶€ì •í™•í•œ ë‹µë³€
            3ì : ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€
            4ì : ëŒ€ë¶€ë¶„ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€
            5ì : ì™„ë²½í•˜ê³  ë§¤ìš° ìœ ìš©í•œ ë‹µë³€
            
            ì ìˆ˜ë§Œ ìˆ«ìë¡œ ë‹µë³€í•´ì£¼ì„¸ìš” (ì˜ˆ: 4)
            """
            
            response = self.llm.invoke(evaluation_prompt)
            quality_score = int(response.content.strip())
            
            improvement_attempts = state.get("improvement_attempts", 0) + 1
            
            return {
                **state,
                "answer_quality_score": quality_score,
                "improvement_attempts": improvement_attempts,
                "current_step": "quality_evaluated"
            }
            
        except Exception as e:
            logger.error(f"ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            return {
                **state,
                "answer_quality_score": 3,  # ê¸°ë³¸ê°’
                "improvement_attempts": 0,
                "current_step": "quality_evaluation_failed"
            }

    def improve_answer(self, state: MeetingQAState) -> MeetingQAState:
        """ë‹µë³€ ê°œì„ """
        try:
            question = state.get("processed_question", "")
            current_answer = state.get("final_answer", "")
            context_chunks = state.get("context_chunks", [])
            quality_score = state.get("answer_quality_score", 3)
            improvement_attempts = state.get("improvement_attempts", 0)
            
            # ê°œì„  ì‹œë„ íšŸìˆ˜ ì¦ê°€
            improvement_attempts += 1
            
            # ì»¨í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
            context_parts = []
            for chunk in context_chunks:
                if "summary_text" in chunk:
                    context_parts.append(f"[ìš”ì•½ë³¸] {chunk.get('summary_text', '')}")
                else:
                    context_parts.append(f"[ì›ë³¸] {chunk.get('chunk_text', '')}")
            
            context = "\n\n".join(context_parts)
            
            # ê°œì„ ëœ ë‹µë³€ ìƒì„±
            improvement_prompt = f"""
            ì´ì „ ë‹µë³€ì˜ í’ˆì§ˆì´ ë‚®ì•˜ìŠµë‹ˆë‹¤ (ì ìˆ˜: {quality_score}/5).
            ë” ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ìœ¼ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”.
            
            ì§ˆë¬¸: {question}
            ì´ì „ ë‹µë³€: {current_answer}
            
            ì°¸ê³  ìë£Œ:
            {context}
            
            ê°œì„ ëœ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”:
            1. ë” êµ¬ì²´ì ì´ê³  ì •í™•í•œ ì •ë³´ ì œê³µ
            2. ì¶œì²˜ ëª…ì‹œ
            3. ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ë‚´ìš©
            4. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ì‘ì„±
            """
            
            response = self.llm.invoke(improvement_prompt)
            improved_answer = response.content.strip()
            
            return {
                **state,
                "final_answer": improved_answer,
                "improvement_attempts": improvement_attempts,
                "current_step": "answer_improved"
            }
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ê°œì„  ì‹¤íŒ¨: {str(e)}")
            return {
                **state,
                "improvement_attempts": improvement_attempts + 1,
                "current_step": "answer_improvement_failed"
            }
    
    def process_question(self, state: MeetingQAState) -> MeetingQAState:
        """1ë‹¨ê³„: ì§ˆë¬¸ ì „ì²˜ë¦¬ ë° í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            user_question = state.get("user_question", "")
            if not user_question:
                raise ValueError("user_questionì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

            # ì§ˆë¬¸ ì „ì²˜ë¦¬ í”„ë¡¬í”„íŠ¸
            question_process_prompt = ChatPromptTemplate.from_template(
                '''ë‹¹ì‹ ì€ íšŒì˜ë¡ ê²€ìƒ‰ì„ ìœ„í•œ ì§ˆë¬¸ì„ ë¶„ì„í•˜ëŠ” AIì…ë‹ˆë‹¤.
                ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë” ëª…í™•í•˜ê³  ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.
                ë‹¨, ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì ˆëŒ€ë¡œ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”. ì§ˆë¬¸ì€ ê¸¸ì–´ì ¸ë„ ê´œì°®ìŠµë‹ˆë‹¤.
                
                ì‚¬ìš©ì ì§ˆë¬¸: {user_question}
                
                ì „ì²˜ë¦¬ëœ ì§ˆë¬¸ì„ ì¶œë ¥í•´ì£¼ì„¸ìš”:'''
            )
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸  
            keyword_extract_prompt = ChatPromptTemplate.from_template(
                '''ë‹¹ì‹ ì€ íšŒì˜ë¡ ê²€ìƒ‰ì„ ìœ„í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤.
                ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ì— ì¤‘ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ 5~8ê°œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
                
                ì§ˆë¬¸: {processed_question}
                
                í•µì‹¬ í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ì¶œë ¥í•´ì£¼ì„¸ìš”:'''
            )

            # ì§ˆë¬¸ ì „ì²˜ë¦¬ ì‹¤í–‰
            formatted_question_prompt = question_process_prompt.format(user_question=user_question)
            question_response = self.llm.invoke(formatted_question_prompt)
            processed_question = question_response.content.strip()

            # í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰
            formatted_keyword_prompt = keyword_extract_prompt.format(processed_question=processed_question)
            keyword_response = self.llm.invoke(formatted_keyword_prompt)
            
            parser = CommaSeparatedListOutputParser()
            search_keywords = parser.parse(keyword_response.content)

            logger.info(f"ì§ˆë¬¸ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(search_keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ")
            
            return {
                **state,
                "processed_question": processed_question,
                "search_keywords": search_keywords,
                "current_step": "question_processed"
            }
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {
                **state,
                "error_message": f"ì§ˆë¬¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                "current_step": "process_question_failed"
            }
    
    def search_rag_summaries(self, state: MeetingQAState) -> MeetingQAState:
        """2ë‹¨ê³„: RAG ì„œë¹„ìŠ¤ì—ì„œ ê´€ë ¨ ìš”ì•½ë³¸ ê²€ìƒ‰"""
        try:
            processed_question = state.get("processed_question", "")
            search_keywords = state.get("search_keywords", [])
            rag_service_url = state.get("rag_service_url", "")
            
            if not all([processed_question, search_keywords, rag_service_url]):
                raise ValueError("í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            rag_client = RAGClient(rag_service_url)
            relevant_summaries = rag_client.search_summaries(
                query=processed_question,
                keywords=search_keywords,
                top_k=5,
                similarity_threshold=0.7
            )
            
            # meeting_id ì¶”ì¶œ
            selected_meeting_ids = list(set([
                summary.get("meeting_id") 
                for summary in relevant_summaries 
                if summary.get("meeting_id")
            ]))
            
            logger.info(f"RAG ê²€ìƒ‰ ì™„ë£Œ: {len(relevant_summaries)}ê°œ ìš”ì•½ë³¸, {len(selected_meeting_ids)}ê°œ íšŒì˜ ID")
            
            return {
                **state,
                "relevant_summaries": relevant_summaries,
                "selected_meeting_ids": selected_meeting_ids,
                "current_step": "rag_search_completed"
            }
            
        except Exception as e:
            logger.error(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return {
                **state,
                "error_message": f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}",
                "current_step": "rag_search_failed"
            }
    
    def fetch_meeting_metadata(self, state: MeetingQAState) -> MeetingQAState:
        """3ë‹¨ê³„: ì™¸ë¶€ íšŒì˜ë¡ APIì—ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
        try:
            selected_meeting_ids = state.get("selected_meeting_ids", [])
            
            if not selected_meeting_ids:
                raise ValueError("ì„ íƒëœ íšŒì˜ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì™¸ë¶€ API í˜¸ì¶œ: GET /api/scripts
            with httpx.Client(timeout=30) as client:
                response = client.get(f"{MEETING_API_URL}/api/scripts")
                
                if response.status_code != 200:
                    raise Exception(f"API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                
                all_meetings = response.json()
                
                # ì„ íƒëœ íšŒì˜ IDë¡œ í•„í„°ë§
                meeting_metadata = [
                    meeting for meeting in all_meetings 
                    if meeting.get("id") in selected_meeting_ids or meeting.get("meeting_id") in selected_meeting_ids
                ]
            
            logger.info(f"ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(meeting_metadata)}ê°œ íšŒì˜ ì •ë³´")
            
            return {
                **state,
                "meeting_metadata": meeting_metadata,
                "current_step": "metadata_fetched"
            }
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {
                **state,
                "error_message": f"ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}",
                "current_step": "fetch_metadata_failed"
            }
    
    def fetch_original_scripts(self, state: MeetingQAState) -> MeetingQAState:
        """4ë‹¨ê³„: ì™¸ë¶€ íšŒì˜ë¡ APIì—ì„œ ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            meeting_metadata = state.get("meeting_metadata", [])
            
            if not meeting_metadata:
                raise ValueError("íšŒì˜ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            original_scripts = []
            
            # ê° íšŒì˜ì— ëŒ€í•´ ìŠ¤í¬ë¦½íŠ¸ ë‚´ìš© ë‹¤ìš´ë¡œë“œ
            with httpx.Client(timeout=30) as client:
                for meeting in meeting_metadata:
                    meeting_id = meeting.get("id") or meeting.get("meeting_id")
                    
                    if not meeting_id:
                        logger.warning(f"íšŒì˜ IDê°€ ì—†ìŠµë‹ˆë‹¤: {meeting}")
                        continue
                    
                    # ì™¸ë¶€ API í˜¸ì¶œ: GET /api/scripts/{id}
                    response = client.get(f"{MEETING_API_URL}/api/scripts/{meeting_id}")
                    
                    if response.status_code == 200:
                        script_content = response.text
                        
                        original_scripts.append({
                            "meeting_id": meeting_id,
                            "content": script_content,
                            "filename": f"meeting_{meeting_id}.txt",
                            "meeting_metadata": meeting
                        })
                    else:
                        logger.warning(f"ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {meeting_id}, ìƒíƒœì½”ë“œ: {response.status_code}")
            
            logger.info(f"ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(original_scripts)}ê°œ íŒŒì¼")
            
            return {
                **state,
                "original_scripts": original_scripts,
                "current_step": "scripts_fetched"
            }
            
        except Exception as e:
            logger.error(f"ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return {
                **state,
                "error_message": f"ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}",
                "current_step": "fetch_scripts_failed"
            }
    
    def process_original_scripts(self, state: MeetingQAState) -> MeetingQAState:
        """5ë‹¨ê³„: ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ ì²­í‚¹ ë° ì„ë² ë”©"""
        try:
            original_scripts = state.get("original_scripts", [])
            
            if not original_scripts:
                raise ValueError("ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            all_chunked_scripts = []
            
            for script in original_scripts:
                meeting_id = script["meeting_id"]
                full_content = script["full_content"]
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                cleaned_content = clean_text(full_content)
                
                # ì²­í‚¹
                chunks = chunk_text(
                    cleaned_content, 
                    chunk_size=DEFAULT_CHUNK_SIZE,
                    chunk_overlap=DEFAULT_CHUNK_OVERLAP
                )
                
                # ì„ë² ë”© ì¶”ê°€
                chunks_with_embeddings = self.embedding_manager.add_embeddings_to_chunks(
                    chunks, meeting_id
                )
                
                all_chunked_scripts.extend(chunks_with_embeddings)
            
            logger.info(f"ìŠ¤í¬ë¦½íŠ¸ ì²˜ë¦¬ ì™„ë£Œ: {len(all_chunked_scripts)}ê°œ ì²­í¬ ìƒì„±")
            
            return {
                **state,
                "chunked_scripts": all_chunked_scripts,
                "current_step": "scripts_processed"
            }
            
        except Exception as e:
            logger.error(f"ìŠ¤í¬ë¦½íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {
                **state,
                "error_message": f"ìŠ¤í¬ë¦½íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                "current_step": "process_scripts_failed"
            }
    
    def select_relevant_chunks(self, state: MeetingQAState) -> MeetingQAState:
        """6ë‹¨ê³„: ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì²­í¬ ì„ ë³„"""
        try:
            processed_question = state.get("processed_question", "")
            chunked_scripts = state.get("chunked_scripts", [])
            
            if not processed_question or not chunked_scripts:
                raise ValueError("í•„ìˆ˜ ë°ì´í„°ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
            query_embedding = self.embedding_manager.embed_query(processed_question)
            
            # ê´€ë ¨ ì²­í¬ ì„ ë³„
            relevant_chunks = find_most_relevant_chunks(
                query_embedding=query_embedding,
                chunks=chunked_scripts,
                top_k=10,
                similarity_threshold=0.6
            )
            
            logger.info(f"ê´€ë ¨ ì²­í¬ ì„ ë³„ ì™„ë£Œ: {len(relevant_chunks)}ê°œ ì²­í¬")
            
            return {
                **state,
                "relevant_chunks": relevant_chunks,
                "current_step": "chunks_selected"
            }
            
        except Exception as e:
            logger.error(f"ì²­í¬ ì„ ë³„ ì‹¤íŒ¨: {str(e)}")
            return {
                **state,
                "error_message": f"ì²­í¬ ì„ ë³„ ì‹¤íŒ¨: {str(e)}",
                "current_step": "select_chunks_failed"
            }
    
    def generate_final_answer(self, state: MeetingQAState) -> MeetingQAState:
        """7ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„±"""
        try:
            user_question = state.get("user_question", "")
            relevant_summaries = state.get("relevant_summaries", [])
            relevant_chunks = state.get("relevant_chunks", [])
            
            if not user_question:
                raise ValueError("ì‚¬ìš©ì ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
            context_parts = []
            
            # ìš”ì•½ë³¸ ì¶”ê°€
            for summary in relevant_summaries[:3]:
                context_parts.append(f"[ìš”ì•½ë³¸] {summary.get('summary_text', '')}")
            
            # ê´€ë ¨ ì²­í¬ ì¶”ê°€
            for chunk in relevant_chunks[:5]:
                context_parts.append(f"[ì›ë³¸] {chunk.get('chunk_text', '')}")
            
            context = "\n\n".join(context_parts)
            
            # ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
            answer_prompt = ChatPromptTemplate.from_template(
                '''ë‹¹ì‹ ì€ íšŒì˜ë¡ ì§ˆì˜ì‘ë‹µ ì „ë¬¸ AIì…ë‹ˆë‹¤. 
                ì£¼ì–´ì§„ íšŒì˜ë¡ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
                
                ì§ˆë¬¸: {question}
                
                ê´€ë ¨ íšŒì˜ë¡ ë‚´ìš©:
                {context}
                
                ë‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:
                1. ì œê³µëœ íšŒì˜ë¡ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”
                2. ì¶”ì¸¡ì´ë‚˜ ê°€ì •ì€ í•˜ì§€ ë§ˆì„¸ìš”
                3. ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ë©´ "ì œê³µëœ íšŒì˜ë¡ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
                4. êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ í•¨ê»˜ ë‹µë³€í•˜ì„¸ìš”
                
                ë‹µë³€:'''
            )
            
            formatted_prompt = answer_prompt.format(
                question=user_question,
                context=context
            )
            
            response = self.llm.invoke(formatted_prompt)
            final_answer = response.content.strip()
            
            # ì¶œì²˜ ì •ë³´ ìƒì„±
            sources = []
            for chunk in relevant_chunks[:5]:
                meeting_metadata = None
                for script in state.get("original_scripts", []):
                    if script["meeting_id"] == chunk["meeting_id"]:
                        meeting_metadata = script.get("metadata", {})
                        break
                
                source = {
                    "meeting_id": chunk["meeting_id"],
                    "meeting_title": meeting_metadata.get("meeting_title", ""),
                    "meeting_date": str(meeting_metadata.get("meeting_date", "")),
                    "chunk_index": chunk["chunk_index"],
                    "relevance_score": chunk["relevance_score"]
                }
                sources.append(source)
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
            confidence_score = min(0.9, max(0.1, 
                sum(chunk["relevance_score"] for chunk in relevant_chunks[:3]) / 3
            )) if relevant_chunks else 0.1
            
            logger.info(f"ë‹µë³€ ìƒì„± ì™„ë£Œ: ì‹ ë¢°ë„ {confidence_score:.2f}")
            
            return {
                **state,
                "context_chunks": context_parts,
                "final_answer": final_answer,
                "sources": sources,
                "confidence_score": confidence_score,
                "current_step": "completed"
            }
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                **state,
                "error_message": f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}",
                "current_step": "generate_answer_failed"
            }
