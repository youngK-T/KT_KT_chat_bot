import os

def load_api_keys(filepath="api_key.txt"):
    with open(filepath, "r") as f:
        for line in f:
            line = line.strip()
            if line and "=" in line:
                key, value = line.split("=", 1)
                os.environ[key.strip()] = value.strip()

path = 'C:/Users/82102/Documents/Job/ALP-B/ALP-B_2nd/chat_bot/'
# API í‚¤ ë¡œë“œ ë° í™˜ê²½ë³€ìˆ˜ ì„¤ì •
load_api_keys(path + 'api_key.txt')


####### ì—¬ëŸ¬ë¶„ì˜ í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ë¥¼ ëª¨ë“œ ì—¬ê¸°ì— ë¶™ì—¬ ë„£ìì‹œë‹¤. #######
## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ---------------------------------------------
import pandas as pd
import numpy as np
import os
import openai
import random
import ast
import fitz
from docx import Document

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

from typing import Annotated, Literal, Sequence, TypedDict, List, Dict
from langchain import hub
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_openai import AzureChatOpenAI
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.output_parsers import CommaSeparatedListOutputParser
from langgraph.graph import StateGraph, END

## ---------------- 1ë‹¨ê³„ : ì‚¬ì „ì¤€ë¹„ ----------------------

# 1) íŒŒì¼ ì…ë ¥ --------------------
def extract_text_from_file(file_path: str) -> str:
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".pdf":
        doc = fitz.open(file_path)
        text = "\n".join(page.get_text() for page in doc)
        doc.close()
        return text
    elif ext == ".docx":
        doc = Document(file_path)
        return "\n".join(p.text for p in doc.paragraphs if p.text.strip())
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF ë˜ëŠ” DOCXë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")

# 2) State ì„ ì–¸ --------------------
class InterviewState(TypedDict):
    # ê³ ì • ì •ë³´
    resume_text: str
    resume_summary: str
    resume_keywords: List[str]
    question_strategy: Dict[str, Dict]

    # ì¸í„°ë·° ë¡œê·¸
    current_question: str
    current_answer: str
    current_strategy: str
    conversation: List[Dict[str, str]]
    evaluation : List[Dict[str, str]]
    next_step : str

# 3) resume ë¶„ì„ --------------------
def analyze_resume(state: InterviewState) -> InterviewState:
    resume_text = state.get("resume_text", "")
    if not resume_text:
        raise ValueError("resume_textê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.")

    llm = AzureChatOpenAI(
        deployment_name=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
        api_version=os.environ["AZURE_OPENAI_API_VERSION"],
        azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
        api_key=os.environ["AZURE_OPENAI_API_KEY"]
    )

    # ìš”ì•½ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    summary_prompt = ChatPromptTemplate.from_template(
        '''ë‹¹ì‹ ì€ ì´ë ¥ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸í„°ë·° ì§ˆë¬¸ì„ ì„¤ê³„í•˜ëŠ” AIì…ë‹ˆë‹¤.
        ë‹¤ìŒ ì´ë ¥ì„œ ë° ìê¸°ì†Œê°œì„œ ë‚´ìš©ì—ì„œ ì§ˆë¬¸ì„ ë½‘ê¸° ìœ„í•œ ì¤‘ìš”í•œ ë‚´ìš©ì„ 10ë¬¸ì¥ ì •ë„ë¡œ ìš”ì•½ì„ í•´ì¤˜(ìš”ì•½ì‹œ ** ê¸°í˜¸ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê²ƒ):\n\n{resume_text}'''
    )
    formatted_summary_prompt = summary_prompt.format(resume_text=resume_text)
    summary_response = llm.invoke(formatted_summary_prompt)
    resume_summary = summary_response.content.strip()

    # í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    keyword_prompt = ChatPromptTemplate.from_template(
        '''ë‹¹ì‹ ì€ ì´ë ¥ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸í„°ë·° ì§ˆë¬¸ì„ ì„¤ê³„í•˜ëŠ” AIì…ë‹ˆë‹¤.
        ë‹¤ìŒ ì´ë ¥ì„œ ë° ìê¸°ì†Œê°œì„œë‚´ìš©ì—ì„œ ì§ˆë¬¸ì„ ë½‘ê¸° ìœ„í•œ ì¤‘ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ 5~10ê°œ ì¶”ì¶œí•´ì¤˜. ë„ì¶œí•œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì¤˜:\n\n{resume_text}'''
    )
    formatted_keyword_prompt = keyword_prompt.format(resume_text=resume_text)
    keyword_response = llm.invoke(formatted_keyword_prompt)

    parser = CommaSeparatedListOutputParser()
    resume_keywords = parser.parse(keyword_response.content)

    return {
        **state,
        "resume_summary": resume_summary,
        "resume_keywords": resume_keywords,
    }

# 4) ì§ˆë¬¸ ì „ëµ ìˆ˜ë¦½ --------------------

def generate_question_strategy(state: InterviewState) -> InterviewState:
    resume_summary = state.get("resume_summary", "")
    resume_keywords = ", ".join(state.get("resume_keywords", []))

    prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì´ë ¥ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸í„°ë·° ì§ˆë¬¸ì„ ì„¤ê³„í•˜ëŠ” AIì…ë‹ˆë‹¤.
ë‹¤ìŒ ì´ë ¥ì„œ ìš”ì•½ê³¼ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ë‹¤ìŒ 3ê°€ì§€ ì§ˆë¬¸ ë¶€ë¬¸ ë³„ë¡œ ì§ˆë¬¸ ë°©í–¥ê³¼ ì˜ˆì‹œ ì§ˆë¬¸ 2ê°œë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì¶œë ¥í•´ì¤˜.

- ì´ë ¥ì„œ ìš”ì•½:
{resume_summary}

- ì´ë ¥ì„œ í‚¤ì›Œë“œ:
{resume_keywords}

ì•„ë˜ í˜•ì‹ì„ ë”°ë¼ì¤˜:
ë”•ì…”ë„ˆë¦¬ í˜•íƒœì—ì„œ keyëŠ” 3ê°€ì§€ ì§ˆë¬¸ ë¶€ë¬¸ì´ì•¼. 'ê²½ë ¥ ë° ê²½í—˜', 'ë™ê¸° ë° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜', 'ë…¼ë¦¬ì  ì‚¬ê³ '
ê° keyì— í•´ë‹¹í•˜ëŠ” valueëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì—¬ê¸°ì˜ keyëŠ” 2ê°€ì§€ë¡œ 'ì§ˆë¬¸ì „ëµ', 'ì˜ˆì‹œì§ˆë¬¸' ì´ì•¼.
'ì§ˆë¬¸ì „ëµ'ì˜ valueëŠ” ê° ì§ˆë¬¸ ë¶€ë¬¸ì— ëŒ€í•´ì„œ, ì´ë ¥ì„œìš”ì•½ ë° í‚¤ì›Œë“œë¡œë¶€í„° ì§ˆë¬¸ ì „ëµ ë° ë°©í–¥ì„ ê²°ì •í•˜ëŠ” êµ¬ì²´ì ì¸ ë¬¸ì¥ì´ì•¼.
'ì˜ˆì‹œì§ˆë¬¸'ì˜ valueëŠ” ë¦¬ìŠ¤íŠ¸í˜•íƒœë¡œ, ì§ˆë¬¸ì „ëµì— ë§ëŠ” êµ¬ì²´ì ì¸ ì˜ˆì‹œ ì§ˆë¬¸ 2ê°œ ë¬¸ì¥ì´ì•¼.

[ì˜ˆì‹œ]
{{{{ "ê²½ë ¥ ë° ê²½í—˜": {{'ì§ˆë¬¸ì „ëµ' : "ì§€ì›ìì˜ ì‹¤ë¬´ ê²½í—˜, ê¸°ìˆ ì  ëŠ¥ë ¥ ë° í”„ë¡œì íŠ¸ ê´€ë¦¬ ê²½í—˜ì„ ì¤‘ì ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§ˆë¬¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì§€ì›ìê°€ ê³¼ê±°ì— ê²½í—˜í•œ ê¸°ìˆ ì  ë„ì „ê³¼ ë¬¸ì œ í•´ê²° ë°©ì‹, íŒ€ ë‚´ì˜ ì—­í•  ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
'ì˜ˆì‹œì§ˆë¬¸': ['KTì˜ AI ì—°êµ¬ì†Œì—ì„œ ì¸í„´ìœ¼ë¡œ ê·¼ë¬´í•˜ë©° OCR ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ ì‹œìŠ¤í…œì„ ê³ ë„í™”í•  ë•Œ ê²ªì—ˆë˜ ê¸°ìˆ ì  ë‚œê´€ì€ ë¬´ì—‡ì´ì—ˆê³ , ì´ë¥¼ ì–´ë–»ê²Œ ê·¹ë³µí–ˆëŠ”ì§€ ì„¤ëª…í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?',
'ë¹…ë°ì´í„° í•™ìƒì—°í•©ì—ì„œ í”„ë¡œì íŠ¸ë¥¼ ë¦¬ë“œí–ˆë˜ ê²½í—˜ì— ëŒ€í•´ ì´ì•¼ê¸°í•´ ì£¼ì„¸ìš”. ì–´ë–¤ ê³¼ì œê°€ ìˆì—ˆê³ , íŒ€ì›ë“¤ê³¼ì˜ ì†Œí†µ ê³¼ì •ì—ì„œ ì–´ë ¤ì›€ì€ ì—†ì—ˆëŠ”ì§€ ê¶ê¸ˆí•©ë‹ˆë‹¤.']}},
"ë™ê¸° ë° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜": ...,
"ë…¼ë¦¬ì  ì‚¬ê³ ": ...
}}}}
ì•ë’¤ë¡œ ```python ~ ``` ë¶™ì´ëŠ”ê²ƒì€ í•˜ì§€ ë§ˆ.
""")

    llm = AzureChatOpenAI(
        deployment_name=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
        api_version=os.environ["AZURE_OPENAI_API_VERSION"],
        azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
        api_key=os.environ["AZURE_OPENAI_API_KEY"]
    )
    formatted_prompt = prompt.format(resume_summary=resume_summary, resume_keywords=resume_keywords)
    response = llm.invoke(formatted_prompt)

    # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    dict_value = response.content.strip()
    if isinstance(dict_value, str):
        try:
            strategy_dict = ast.literal_eval(dict_value)
        except Exception as e:
            raise ValueError("question_strategyë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.") from e

    return {
        **state,
        "question_strategy": strategy_dict
    }

# 5) 1ë‹¨ê³„ í•˜ë‚˜ë¡œ ë¬¶ê¸° --------------------

def preProcessing_Interview(file_path: str) -> InterviewState:
    # íŒŒì¼ ì…ë ¥
    resume_text = extract_text_from_file(file_path)

    # state ì´ˆê¸°í™”
    initial_state: InterviewState = {
        "resume_text": resume_text,
        "resume_summary": '',
        "resume_keywords": [],
        "question_strategy": {},

        "current_question": '',
        "current_answer": '',
        "current_strategy": '',
        "conversation": [],
        "evaluation": [],
        "next_step" : ''
    }

    # Resume ë¶„ì„
    state = analyze_resume(initial_state)

    # ì§ˆë¬¸ ì „ëµ ìˆ˜ë¦½
    state = generate_question_strategy(state)

    # ì²«ë²ˆì§¸ ì§ˆë¬¸ ìƒì„±
    question_strategy = state["question_strategy"]
    example_questions = question_strategy["ê²½ë ¥ ë° ê²½í—˜"]["ì˜ˆì‹œì§ˆë¬¸"]
    selected_question = random.choice(example_questions)

    return {
            **state,
            "current_question": selected_question,
            "current_strategy": "ê²½ë ¥ ë° ê²½í—˜"
            }


## ---------------- 2ë‹¨ê³„ : ë©´ì ‘ Agent ----------------------

# 1) ë‹µë³€ ì…ë ¥ --------------------
def update_current_answer(state: InterviewState, user_answer: str) -> InterviewState:
    return {
        **state,
        "current_answer": user_answer.strip()
    }

# 2) ë‹µë³€ í‰ê°€ --------------------
def evaluate_answer(state: InterviewState) -> InterviewState:
    llm = AzureChatOpenAI(
        deployment_name=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
        api_version=os.environ["AZURE_OPENAI_API_VERSION"],
        azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
        api_key=os.environ["AZURE_OPENAI_API_KEY"]
    )

    current_question = state.get("current_question", "")
    current_answer = state.get("current_answer", "")
    current_strategy = state.get("current_strategy", "")
    question_strategy = state.get("question_strategy", {})
    resume_summary = state.get("resume_summary", "")
    resume_keywords = ", ".join(state.get("resume_keywords", []))

    # ì§ˆë¬¸ ì „ëµ ì¶”ì¶œ
    strategy_block = ""
    if isinstance(question_strategy, dict):
        strategy_block = question_strategy.get(current_strategy, {}).get("ì§ˆë¬¸ì „ëµ", "")
    elif isinstance(question_strategy, str):
        try:
            parsed = ast.literal_eval(question_strategy)
            strategy_block = parsed.get(current_strategy, {}).get("ì§ˆë¬¸ì „ëµ", "")
        except Exception:
            strategy_block = ""

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì¸í„°ë·° í‰ê°€ë¥¼ ìœ„í•œ AI í‰ê°€ìì…ë‹ˆë‹¤.
ë‹¤ìŒì˜ ì°¸ì¡° ì •ë³´(ì§€ì›ìì˜ ì´ë ¥ì„œ ìš”ì•½, í‚¤ì›Œë“œ, ì§ˆë¬¸ ì „ëµ, í˜„ì¬ ì§ˆë¬¸ê³¼ ë‹µë³€)ë¥¼ í™•ì¸í•˜ì„¸ìš”.
[ì°¸ê³  ì •ë³´]
- ì´ë ¥ì„œ ìš”ì•½: {resume_summary}
- ì´ë ¥ì„œ í‚¤ì›Œë“œ: {resume_keywords}
- ì§ˆë¬¸ ì „ëµ({current_strategy}): {strategy}
- ì§ˆë¬¸: {question}
- ë‹µë³€: {answer}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ ë‘ ê°€ì§€ í•­ëª©ì— ë”°ë¼ [ë‹µë³€]ì„ í‰ê°€í•´ ì£¼ì„¸ìš”.
- [ë‹µë³€]ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ ìˆìœ¼ë©´ ë‚®ê²Œ í‰ê°€.
í•­ëª© í‰ê°€ ê¸°ì¤€
- ì§ˆë¬¸ê³¼ì˜ ì—°ê´€ì„±: ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆëŠ”ì§€ í‰ê°€í•´ ì£¼ì„¸ìš”.
- ë‹µë³€ì˜ êµ¬ì²´ì„±: ë‹µë³€ì´ ì–¼ë§ˆë‚˜ êµ¬ì²´ì ì´ê³  ì‹¤ì§ˆì ì¸ ì˜ˆì‹œë‚˜ ê²½í—˜ ë° ì„¤ëª…ì„ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ë¥¼ í‰ê°€í•´ ì£¼ì„¸ìš”.
ê° í•­ëª©ì— ëŒ€í•´ 'ìƒ', 'ì¤‘', 'í•˜' ì¤‘ í•˜ë‚˜ë¡œ í‰ê°€í•´ ì£¼ì„¸ìš”.
- ìƒ : ìš°ìˆ˜, ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ì— ì •í™•íˆ ë¶€í•©í•˜ë©°, ì „ë°˜ì ì¸ ë‚´ìš©ì„ ëª…í™•íˆ ë‹¤ë£¸
- ì¤‘ : ë³´í†µ, ì§ˆë¬¸ê³¼ ê´€ë ¨ì€ ìˆì§€ë§Œ í•µì‹¬ í¬ì¸íŠ¸ê°€ ë¶€ë¶„ì ìœ¼ë¡œ ëˆ„ë½ë¨
- í•˜ : ë¯¸í¡, ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì•½í•˜ê±°ë‚˜ ì—‰ëš±í•œ ë‚´ìš© ì¤‘ì‹¬

ìµœì¢… ê²°ê³¼ëŠ” ì•„ë˜ í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬ë¡œë§Œ ì¶œë ¥í•´ ì£¼ì„¸ìš”
ì¶œë ¥ ì˜ˆì‹œ :
{{
  "ì§ˆë¬¸ê³¼ì˜ ì—°ê´€ì„±": "ìƒ",
  "ë‹µë³€ì˜ êµ¬ì²´ì„±": "ì¤‘"
}}
""")

    formatted_prompt = prompt.format(
        resume_summary=resume_summary,
        resume_keywords=resume_keywords,
        strategy=strategy_block,
        current_strategy=current_strategy,
        question=current_question,
        answer=current_answer
    )

    response = llm.invoke(formatted_prompt)
    # print(response.content.strip())
    # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    eval_dict = response.content.strip()
    if isinstance(eval_dict, str):
        try:
            eval_dict = ast.literal_eval(eval_dict)
        except Exception as e:
            raise ValueError("question_strategyë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.") from e

    # (1) ëŒ€í™” ì €ì¥ (ì§ˆë¬¸/ë‹µë³€ 1ìŒ)
    state["conversation"].append({"question": current_question, "answer": current_answer})

    # (2) í‰ê°€ ì €ì¥ (ì¸ë±ìŠ¤ í¬í•¨)
    evaluation = state.get("evaluation", [])
    eval_dict["question_index"] = len(state["conversation"]) - 1
    evaluation.append(eval_dict)

    return {
        **state,
        "evaluation": evaluation
    }

# 3) ì¸í„°ë·° ì§„í–‰ ê²€í†  --------------------
def decide_next_step(state: InterviewState) -> InterviewState:
    evaluation = state.get("evaluation", [])
    conversation = state.get("conversation", [])

    # (1) ì§ˆë¬¸-ë‹µë³€ ìˆ˜ê°€ 3íšŒë¥¼ ì´ˆê³¼í•˜ë©´ ì¢…ë£Œ
    if len(conversation) > 3:
        next_step = "end"

    # (2) ê°€ì¥ ìµœê·¼ í‰ê°€ì—ì„œ 'í•˜'ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì¶”ê°€ ì§ˆë¬¸
    else :
        next_step = "additional_question"

    return {
        **state,
        "next_step": next_step
    }

# 4) ì§ˆë¬¸ ìƒì„± --------------------
def generate_question(state: InterviewState) -> InterviewState:
    llm = AzureChatOpenAI(
        deployment_name=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
        api_version=os.environ["AZURE_OPENAI_API_VERSION"],
        azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
        api_key=os.environ["AZURE_OPENAI_API_KEY"]
    )
    resume_summary = state.get("resume_summary", "")
    resume_keywords = ", ".join(state.get("resume_keywords", []))
    question_strategy = state.get("question_strategy", {})
    current_strategy = state.get("current_strategy", "")
    stragety = question_strategy[current_strategy]['ì§ˆë¬¸ì „ëµ']
    current_question = state.get("current_question", "")
    current_answer = state.get("current_answer", "")
    evaluation = state.get("evaluation", [])

    last_evaluation = evaluation[-1] if evaluation else {}

    # ì‹¬í™”(ì¶”ê°€) ì§ˆë¬¸
    prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì¸í„°ë·° ì§ˆë¬¸ì„ ì„¤ê³„í•˜ëŠ” AIì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ì¶”ê°€ ì§ˆë¬¸ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì°¸ì¡°í•  ì¤‘ìš”í•œ ì •ë³´ì…ë‹ˆë‹¤.
- ì´ë ¥ì„œ ìš”ì•½: {resume_summary}
- ì´ë ¥ì„œ í‚¤ì›Œë“œ: {resume_keywords}
- ì§ˆë¬¸ ì „ëµ({current_strategy}): {strategy}
- ì´ì „ ì§ˆë¬¸: {current_question}
- ë‹µë³€: {current_answer}
- í‰ê°€: {evaluation}

ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§€ì›ìì˜ ì‚¬ê³ ë ¥, ë¬¸ì œ í•´ê²° ë°©ì‹, í˜¹ì€ ê¸°ìˆ ì  ê¹Šì´ë¥¼ ë” í™•ì¸í•  ìˆ˜ ìˆëŠ” ì‹¬í™” ì¸í„°ë·° ì§ˆë¬¸ì„ í•œ ê°€ì§€ ìƒì„±í•´ì£¼ì„¸ìš”.
êµ¬ì²´ì ì´ê³ , ì§€ì›ìì˜ ëŒ€ë‹µì„ í™•ì¥ì‹œí‚¬ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ ë‚ ì¹´ë¡œìš´ ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ì§ˆë¬¸ì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
""")

    formatted_prompt = prompt.format(
        current_question=current_question,
        current_answer=current_answer,
        evaluation=str(last_evaluation),
        resume_summary=resume_summary,
        resume_keywords=resume_keywords,
        strategy=stragety,
        current_strategy=current_strategy
    )

    response = llm.invoke(formatted_prompt)

    return {
        **state,
        "current_question": response.content.strip(),
        "current_answer": ""
    }

# 5) ì¸í„°ë·° í”¼ë“œë°± ë³´ê³ ì„œ --------------------
def summarize_interview(state: InterviewState) -> InterviewState:
    print("\n ì¸í„°ë·° ì¢…ë£Œ ë³´ê³ ì„œ")
    print("-" * 50)
    for i, turn in enumerate(state["conversation"]):
        print(f"[ì§ˆë¬¸ {i+1}] {turn['question']}")
        print(f"[ë‹µë³€ {i+1}] {turn['answer']}")
        if i < len(state["evaluation"]):
            eval_result = state["evaluation"][i]
            print(f"[í‰ê°€] ì§ˆë¬¸ê³¼ì˜ ì—°ê´€ì„±: {eval_result['ì§ˆë¬¸ê³¼ì˜ ì—°ê´€ì„±']}, ë‹µë³€ì˜ êµ¬ì²´ì„±: {eval_result['ë‹µë³€ì˜ êµ¬ì²´ì„±']}")
        print("-" * 50)
    return state

# 6) Agent --------------------
# ë¶„ê¸° íŒë‹¨ í•¨ìˆ˜
def route_next(state: InterviewState) -> Literal["generate", "summarize"]:
    return "summarize" if state["next_step"] == "end" else "generate"

# ê·¸ë˜í”„ ì •ì˜ ì‹œì‘
builder = StateGraph(InterviewState)

# ë…¸ë“œ ì¶”ê°€
builder.add_node("evaluate", evaluate_answer)
builder.add_node("decide", decide_next_step)
builder.add_node("generate", generate_question)
builder.add_node("summarize", summarize_interview)

# ë…¸ë“œ ì—°ê²°
builder.set_entry_point("evaluate")
builder.add_edge("evaluate", "decide")
builder.add_conditional_edges("decide", route_next)
builder.add_edge("generate", END)      # ë£¨í”„
builder.add_edge("summarize", END)            # ì¢…ë£Œ

# ì»´íŒŒì¼
graph = builder.compile()
#-------------------------------------------------------------------


########### ë‹¤ìŒ ì½”ë“œëŠ” ì œê³µë˜ëŠ” gradio ì½”ë“œ ì…ë‹ˆë‹¤.################

import gradio as gr
import tempfile

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” í•¨ìˆ˜
def initialize_state():
    return {
        "state": None,
        "interview_started": False,
        "interview_ended": False,
        "chat_history": []
    }

# íŒŒì¼ ì—…ë¡œë“œ í›„ ì¸í„°ë·° ì´ˆê¸°í™”
def upload_and_initialize(file_obj, session_state):
    if file_obj is None:
        return session_state, "íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."

    # GradioëŠ” file_obj.name ì´ íŒŒì¼ ê²½ë¡œì•¼
    file_path = file_obj.name

    # ì¸í„°ë·° ì‚¬ì „ ì²˜ë¦¬
    state = preProcessing_Interview(file_path)
    session_state["state"] = state
    session_state["interview_started"] = True

    # ì²« ì§ˆë¬¸ ì €ì¥
    first_question = state["current_question"]
    session_state["chat_history"].append(["ğŸ¤– AI ë©´ì ‘ê´€", first_question])

    return session_state, session_state["chat_history"]

# ë‹µë³€ ì²˜ë¦¬ ë° ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±
def chat_interview(user_input, session_state):
    if not session_state["interview_started"]:
        return session_state, "ë¨¼ì € ì´ë ¥ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸í„°ë·°ë¥¼ ì‹œì‘í•˜ì„¸ìš”."

    # (1) ì‚¬ìš©ì ë‹µë³€ ì €ì¥
    session_state["chat_history"].append(["ğŸ™‹â€â™‚ï¸ ì§€ì›ì", user_input])
    session_state["state"] = update_current_answer(session_state["state"], user_input)

    # (2) Agent ì‹¤í–‰ (í‰ê°€ ë° ë‹¤ìŒ ì§ˆë¬¸ or ì¢…ë£Œ)
    session_state["state"] = graph.invoke(session_state["state"])

    # (3) ì¢…ë£Œ ì—¬ë¶€ íŒë‹¨
    if session_state["state"]["next_step"] == "end":
        session_state["interview_ended"] = True
        final_summary = "âœ… ì¸í„°ë·°ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n\n"
        for i, turn in enumerate(session_state["state"]["conversation"]):
            final_summary += f"\n**[ì§ˆë¬¸ {i+1}]** {turn['question']}\n**[ë‹µë³€ {i+1}]** {turn['answer']}\n"
            if i < len(session_state["state"]["evaluation"]):
                eval_result = session_state["state"]["evaluation"][i]
                final_summary += f"_í‰ê°€ - ì§ˆë¬¸ ì—°ê´€ì„±: {eval_result['ì§ˆë¬¸ê³¼ì˜ ì—°ê´€ì„±']}, ë‹µë³€ êµ¬ì²´ì„±: {eval_result['ë‹µë³€ì˜ êµ¬ì²´ì„±']}_\n"
        session_state["chat_history"].append(["ğŸ¤– AI ë©´ì ‘ê´€", final_summary])
        return session_state, session_state["chat_history"], gr.update(value="")

    else:
        next_question = session_state["state"]["current_question"]
        session_state["chat_history"].append(["ğŸ¤– AI ë©´ì ‘ê´€", next_question])
        return session_state, session_state["chat_history"], gr.update(value="")

# Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±
with gr.Blocks() as demo:
    session_state = gr.State(initialize_state())

    gr.Markdown("# ğŸ¤– AI ë©´ì ‘ê´€ \nì´ë ¥ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸í„°ë·°ë¥¼ ì‹œì‘í•˜ì„¸ìš”!")

    with gr.Row():
        file_input = gr.File(label="ì´ë ¥ì„œ ì—…ë¡œë“œ (PDF ë˜ëŠ” DOCX)")
        upload_btn = gr.Button("ì¸í„°ë·° ì‹œì‘")

    chatbot = gr.Chatbot()
    user_input = gr.Textbox(show_label=False, placeholder="ë‹µë³€ì„ ì…ë ¥í•˜ê³  Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")

    upload_btn.click(upload_and_initialize, inputs=[file_input, session_state], outputs=[session_state, chatbot])
    user_input.submit(chat_interview, inputs=[user_input, session_state], outputs=[session_state, chatbot])
    user_input.submit(lambda: "", None, user_input)

# ì‹¤í–‰
demo.launch(share=True)